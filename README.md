# Glossary

Here, we collect definitions of important concepts

**Random forest**: A machine learning algorithm for classification, regression and other tasks that operates by constructing a multitude of decision trees (during training) to generate a single result.

**Data dimensionality**: Dimensionality of data refers to the number of properties or categories in a dataset

**Regression Analysis**: Statistical processes for estimating the relationships between a dependent variable and one or more independent variables.

**Black box**: The inability to undestand how a deep learning system makes its decisions.

**Deep Learning**: neural networks with multiple layers, allowing complex patterns and representations to be learned from large datasets.

**Signal** The part of the data you want to study/explore. Datasets must sometimes be denoised in order to pull out what you want.

**Local model interpretation:** set of techniques aimed at evaluating why a model makes a specific prediction and what effect a specific feature value has on the prediction. 

**Data Mining:** Analyzing raw data to extract useful information and find patterns.

**Structured Data**: data organized in a specific format, such as position (lat, long) information or physical measurements.

**Unstructured Data**: data that are not organized in a specific format and require more complex methods for automated interpretation. Examples include images, papers, books, audio files etc.

**Data modality:** The simplest structure in which your data can be described (e.g., row-column, multi-dimensional, raster/image). 

**K Means Clustering** A machine learning algorithm for grouping variables that partitions the signals of interest by their behavior/patterns. This is an example of an unsupervised machine learning model. 

**Logistic Regression:** A statistical model used for classification given a dataset. Provides a discrete output.

**Node:** refers to computing unit a single CPU with multiple cores, a dedicated memory, and a dedicated storage. Terminology used in high performance computing (HPC).

**Instance:** refers to a computing unit, like a node. Terminology used in Cloud.

**Core:** a processing unit of a CPU or GPU that performs the tasks.

**Dimensionality Reduction:** A data transformation technique where high-dimensionality data is converted to lower-dimensionality data while preserving as much meaningful structure in the data as possible to address the curse of dimensionality and improve data visualization, interpretation, clustering, and other processing (ex. machine learning). 

![image](https://github.com/UW-ESS-DS/Glossary/assets/56406566/7477fad4-895e-4a42-97b6-40b3bcc2bf0a)

**Feature Engineering:** Manipulating raw data to select specific features from within the data.  Also referred to as feature extraction or feature discovery.  A feature, sometimes used interchangably with the term variable, can be a characteristic, property, attribute, etc. within the raw data.

**Deep Learning:** a subfield in machine learning based on artificial neural networks in which multiple layers of processing are used to extract progressively higher level features from data.
